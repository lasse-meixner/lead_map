---
title: "Understanding identification in pogit from the ground up pt. 2"
format:
    html:
      code-fold: true
---

Part 1 covered some first modifications of the most simple pogit in search of what breaks our model. As long as the data was normally distributed, everything sees fine unless one introduces nearly-perfect multicollinearity in the logit. This is of course no meaningful insight since we know this to break identification in any index.

In the real data, we also have identification issues even if the logit variables are few and barely correlated at all. So we must look elsewhere...

My other suspicion is that identification in the real data fails because there is a high concentration of data points for which the logit probability has sufficiently little curvature. My intuition was that this **would create something like a multicollinearity problem** in the rate product $\mu_i \cdot \pi_i$ for the thinned poisson rate.

<!-- add notes here -->

This chapter will dive into the dgp-related questions that could give rise to this phenomenon.

The second derivative of the logistic is

$$\frac{d^2}{dx^2} \text{logistic}(x) = -\frac{e^x(e^x - 1)}{(e^x + 1)^3}$$

which tends to 0 when the index (which measure the *log odds*) $x$ goes to either $-\infty$ or $\infty$ and is zero at $x=0$. For these values, the curvature is small and the function therefore approximately linear. This corresponds to probabilities approaching 0 or 1 respectively, or those very close to 0.5.

```{r, echo=FALSE}
library(dplyr)
library(ggplot2)
# plot the logistic and its second derivative
x_seq <- seq(-10, 10, length.out = 1000)
logistic <- function(x) 1 / (1 + exp(-x))
d2_logistic <- function(x) -exp(x) * (exp(x) - 1) / (exp(x) + 1)^3

logistic_v  <- logistic(x_seq)
d2_logistic_v <- d2_logistic(x_seq)

data.frame(x = x_seq, logistic_v = logistic_v, d2_logistic_v = d2_logistic_v) |>
  ggplot(aes(x = x)) +
  geom_line(aes(y = logistic_v), color = "black") +
  geom_line(aes(y = d2_logistic_v), color = "grey") +
  labs(title = "Logistic and its second derivative",
       x = "x",
       y = "value")
```

Let us start by simulating the same data as in pt. 1 and have a look at how the $\pi_i$ values, which are the thinning probabilities, are distributed.

```{r}
# data simulation as in Ch1
set.seed(1999)
library(mvtnorm)

n <- 1000
rho <- 0.5
S <- matrix(c(1, rho,
              rho, 1), 2, 2, byrow = TRUE)
x_z <- rmvnorm(n, sigma = S)
x <- x_z[,1] 
z <- x_z[,2] 

alpha <- 0.5 # poisson intercept
beta <- 1 # x in poisson
log_mu <- alpha + beta * (x - mean(x)) # poisson rate
ystar <- rpois(n, exp(log_mu)) # unobserved count

gamma <- (-0.5) # logit intercept
delta <- 1.2 # excluded z in logit
kappa <- 0.5 # x in logit
logit_pi <- gamma + delta * (z - mean(z)) + kappa * (x - mean(x)) # logit probability
y <- rbinom(n, size = ystar, prob = plogis(logit_pi)) # observed (thinned) count

true_params <- c(alpha = alpha, beta = beta, gamma = gamma, delta = delta, kappa = kappa)

```

We saw that under this DGP, there are no model problems and sampling is efficient.

Here our thinning probabilities are distributed as follows:

```{r}
library(ggplot2)
library(dplyr)

x_seq <- seq(min(logit_pi), max(logit_pi), length.out = 1000)
d2_logistic_df <- data.frame(x = x_seq, d2_logistic_v = d2_logistic(x_seq), logit_pi = logit_pi)

d2_logistic_df |>
    ggplot() +
    geom_histogram(aes(x = logit_pi), bins = 30, fill = "grey") +
    geom_line(aes(x = x, y = d2_logistic_v*300), color = "black") +
    labs(title = "Distribution of thinning probabilities (log odds)",
             x = "log odds",
             y = "count")
```

This implies the following distribution of thinning probabilities:

```{r}
hist(plogis(logit_pi))
```

As a heuristic metric, let us consider the (empirical) expected curvature of the the logistic function at the thinning probabilities.

$$\frac{1}{n} \sum_{i=1}^n -\frac{e^{\pi_i}(e^{\pi_i} - 1)}{(e^{\pi_i} + 1)^3}$$

```{r}
mean(d2_logistic(logit_pi))
```

Let's now make this more ugly: We modify the DGP such that the thinning probabilities in our samples are more concentrated around zero and 1 (the sampling chains in for the real data draw mostly ones - although the chains do not mix well...).

I will first leave x nicely normally distributed, and just crank up its influence on the logit part of the model to push the thinning probabilities towards 0 and 1.

```{r}
kappa_new <- 3 # x in logit
```

Creating new thinning probabilities:

```{r}
logit_pi_new <- gamma + delta * (z - mean(z)) + kappa_new * (x - mean(x))
probs <- plogis(logit_pi_new)
hist(probs)
```

and has a much lower expected curvature:

```{r}
mean(d2_logistic(logit_pi_new))
```

Let us now compare estimation. I ignore the censoring which matters only for computational matters but not the core of the identification problem.

```{r}
library(cmdstanr)

# load model
stan_model <- cmdstan_model("thinned_poisson_w_priors_x_logit.stan")

# original data
dat <- list(N = n, y = y, x = x, z = z, alpha_prior_var = 0.1)

# fit
fit <- stan_model$sample(data = dat, chains = 4, parallel_chains = 4, refresh = 500)
```

```{r, echo=FALSE}
library(bayesplot)
library(ggplot2)

# auxiliary function to plot diagnostics (posterior draws, trace plots)

plot_diagnostics <- function(fit){
    draws <- fit$draws(format = "draws_df") |>
        dplyr::select(-lp__)

    fig1 <- mcmc_areas(draws, prob = 0.9) + ggtitle("Posterior draws") +
        theme_minimal()

    fig2 <- mcmc_trace(draws) + ggtitle("Trace plots") + theme_minimal()

    fig3 <- mcmc_rank_ecdf(draws, prob = 0.99, plot_diff = TRUE) + ggtitle("Cumulative rank plots (difference)") +
    theme_minimal()

    # potentially add plot for divergent transitions

    list(fig1, fig2, fig3)
}
```

```{r}
plot_diagnostics(fit)
```

Now we generate data with the new DGP and estimate the model again.

```{r}
# new data
y_new <- rbinom(n, size = ystar, prob = plogis(logit_pi_new))
dat_new <- list(N = n, y = y_new, x = x, z = z, alpha_prior_var = 0.1)

# fit
fit_new <- stan_model$sample(data = dat_new, chains = 4, parallel_chains = 4, refresh = 500)

plot_diagnostics(fit_new)
```

This picture confirms the problem! The posteriors for x are not identified on either side of the model, and one chain is stuck in a completely different region with a higher beta and lower kappa. This is reflected by poor convergence diagnostics:

```{r}
fit_new$summary() |>
    knitr::kable(digits = 2)
```

We can also push this further to the extreme: Let's make the thinning probabilities even more concentrated around 0 and 1.

```{r}
kappa_new_2 <- 10 # x in logit
logit_pi_new_2 <- gamma + delta * (z - mean(z)) + kappa_new_2 * (x - mean(x))
probs_new_2 <- plogis(logit_pi_new_2)
hist(probs_new_2)
```

Average curvature is now even lower:

```{r}
mean(d2_logistic(logit_pi_new_2))
```

1.  Estimating this should be even worse:

```{r}
y_new_2 <- rbinom(n, size = ystar, prob = plogis(logit_pi_new_2))
dat_new_2 <- list(N = n, y = y_new_2, x = x, z = z, alpha_prior_var = 0.1)

fit_new_2 <- stan_model$sample(data = dat_new_2, chains = 4, parallel_chains = 4, refresh = 500)

fit_new_2$summary() |>
    knitr::kable(digits = 2)
```

```{r}
plot_diagnostics(fit_new_2)
```

::: callout-note
For some very odd reason whenever this is run by commamd this gives very different results than the output from rendering the html file... Because of different initializations? I tried this about 2 times and it is consistenty different. It is possible that for even extremer values of $\kappa$ the extremism might reduce stochastic variability in a way that makes the model converge better again, this I have to figure out as well. But it doesnt make sense that the results depend from where the code is run(!). I need to fix this \[#todo:\].
:::

I think that this is the phenomenon behind the identification issues in the real data. Identification there fails because the chains mostly draw *values that imply thinning probabilities of close to 1* where the logistic function is approximately linear.

In other words, the model breaks down if thinning is **not substantially happening** for a sufficiently large number of data points.

In the next chapter, I will think about what this implies about the problem we are trying to model and how one could fix it (and what a fix would even mean).
